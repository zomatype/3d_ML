import pandas as pd
import numpy as np
import tensorflow as tf
from sklearn.model_selection import train_test_split
from tensorflow.keras.utils import Sequence
import os
import gc

# â–¼â–¼â–¼ è¨­å®š â–¼â–¼â–¼
# è¢«é¨“è€…å(ãƒ•ã‚¡ã‚¤ãƒ«åã«ä½¿ç”¨)
NAME = "okabe"  # ä¾‹: "soma" 

# ãƒ‘ã‚¹ã®è‡ªå‹•ç”Ÿæˆ
INPUT_FILE_PATH = f"inputs/input_{NAME}.csv"
RESULT_FILE_PATH = f"inputs/result_{NAME}.csv"
SAVE_MODEL_PATH = f"models/model_finetuned_{NAME}.keras"

# Zè»¸ï¼ˆçŸ­è¾ºï¼‰: ãƒ‡ãƒ¼ã‚¿æœ€å¤§å€¤ 846.5mm ã‚’ã‚«ãƒãƒ¼ã™ã‚‹ãŸã‚ã€ä½™è£•ã‚’æŒã£ã¦ Â±1000mm ã«è¨­å®š
Z_RANGE = (-1000.0, 1000.0)  # å¹… 2000mm

# Xè»¸ï¼ˆé•·è¾ºï¼‰: Zè»¸ã®å¹…(2000mm) ã®ã€Œ2å€ã€ã®å¹…ã‚’ç¢ºä¿ã—ã¦ã€æ¯”ç‡2:1ã‚’ç¶­æŒ
X_RANGE = (-2000.0, 2000.0)  # å¹… 4000mm (-2000 ~ 2000)
MAP_SIZE = (64, 32)
TIME_STEPS = 20
MODEL_PATH = "models/model_base.keras"

# å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã®ä½¿ç”¨å‰²åˆ (0.0 ã«ã™ã‚‹ã¨å­¦ç¿’ãªã—ã§è©•ä¾¡ã®ã¿)
FINE_TUNE_RATIO = 0.2  # 20%
EPOCHS = 15
LEARNING_RATE = 0.0001

# â–¼â–¼â–¼ æ­£è¦åŒ–ã®é©ç”¨ â–¼â–¼â–¼
# ã€æˆ¦ç•¥é¸æŠã€‘
# STRATEGY = "Global"  # Aã•ã‚“ã®çµ±è¨ˆé‡ã‚’ä½¿ã† (å¾“æ¥æ‰‹æ³•)
STRATEGY = "SubjectSpecific"  # Bã•ã‚“ã®çµ±è¨ˆé‡ã‚’ä½¿ã† (ææ¡ˆæ‰‹æ³•: ã‚­ãƒ£ãƒªãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³)
# â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²

class HeatmapGenerator(Sequence):
    def __init__(self, x_set, z_coords, batch_size, map_size, x_range, z_range, sigma=1.5):
        self.x = x_set
        self.z = z_coords
        self.batch_size = batch_size
        
        # map_size=(64, 32) ã¯ (Width, Height)
        self.w, self.h = map_size 
        
        self.x_range = x_range
        self.z_range = z_range
        self.sigma = sigma
        self.indices = np.arange(len(self.x))

        # ã‚°ãƒªãƒƒãƒ‰ä½œæˆ (Shape: Height x Width)
        x_grid = np.arange(0, self.w, 1, np.float32)
        z_grid = np.arange(0, self.h, 1, np.float32)
        self.X_grid, self.Z_grid = np.meshgrid(x_grid, z_grid)

    def __len__(self):
        return int(np.ceil(len(self.x) / self.batch_size))

    def __getitem__(self, idx):
        inds = self.indices[idx * self.batch_size:(idx + 1) * self.batch_size]
        batch_x = self.x[inds]
        batch_z_coords = self.z[inds]
        batch_heatmaps = self._generate_heatmaps(batch_z_coords)
        return batch_x, batch_heatmaps

    def _generate_heatmaps(self, coords):
        batch_size = len(coords)
        num_keypoints = coords.shape[1] // 2
        
        # shape: (Batch, Height, Width, Channels) -> (B, 32, 64, 6)
        # ãƒ¢ãƒ‡ãƒ«ã®å‡ºåŠ›å½¢çŠ¶ (32, 64) ã«åˆã‚ã›ã‚‹
        heatmaps = np.zeros((batch_size, self.h, self.w, num_keypoints), dtype=np.float32)

        min_x, max_x = self.x_range
        min_z, max_z = self.z_range

        for i in range(batch_size):
            flat_coords = coords[i]
            for k in range(num_keypoints):
                real_x = flat_coords[k*2]
                real_z = flat_coords[k*2 + 1]

                # ã‚¯ãƒªãƒƒãƒ”ãƒ³ã‚°
                if real_x < min_x: real_x = min_x
                if real_x > max_x: real_x = max_x
                if real_z < min_z: real_z = min_z
                if real_z > max_z: real_z = max_z

                norm_x = (real_x - min_x) / (max_x - min_x)
                norm_z = (real_z - min_z) / (max_z - min_z)

                center_x = norm_x * (self.w - 1)
                center_z = norm_z * (self.h - 1)

                d2 = (self.X_grid - center_x)**2 + (self.Z_grid - center_z)**2
                g = np.exp(-d2 / (2 * self.sigma**2))
                heatmaps[i, :, :, k] = g

        return heatmaps

    def on_epoch_end(self):
        np.random.shuffle(self.indices)

def get_coords_logic5(heatmap, x_range, z_range):
    h, w = heatmap.shape
    threshold = np.max(heatmap) * 0.2
    heatmap_thresh = np.where(heatmap > threshold, heatmap, 0)
    total_mass = np.sum(heatmap_thresh)
    
    if total_mass <= 1e-6:
        y_idx, x_idx = np.unravel_index(np.argmax(heatmap), (h, w))
        center_w, center_h = x_idx, y_idx
    else:
        x_grid = np.arange(w)
        y_grid = np.arange(h)
        X_mesh, Y_mesh = np.meshgrid(x_grid, y_grid)
        center_w = np.sum(X_mesh * heatmap_thresh) / total_mass
        center_h = np.sum(Y_mesh * heatmap_thresh) / total_mass
    
    norm_w = center_w / (w - 1)
    norm_h = center_h / (h - 1)
    
    # Logic 5
    pred_x = x_range[0] + norm_w * (x_range[1] - x_range[0])
    pred_z = z_range[0] + norm_h * (z_range[1] - z_range[0])
    return pred_x, pred_z

def main():
    if not os.path.exists(MODEL_PATH):
        print(f"âŒ ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ãªã—: {MODEL_PATH}")
        print("rssi_to_pose_heatmap.py ã‚’å®Ÿè¡Œã—ã¦ãƒ¢ãƒ‡ãƒ«ã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚")
        return
    
    print(f"ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿: {MODEL_PATH}")
    model = tf.keras.models.load_model(MODEL_PATH)
    opt = tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE, clipnorm=1.0)
    model.compile(optimizer=opt, loss='mse')

    print(f"æ¤œè¨¼ç”¨ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ (ID: {NAME})...")
    try:
        # è¨­å®šã§æŒ‡å®šã—ãŸãƒ‘ã‚¹ã‚’ä½¿ç”¨
        print(f" - Input: {INPUT_FILE_PATH}")
        print(f" - Result: {RESULT_FILE_PATH}")
        
        input_df = pd.read_csv(INPUT_FILE_PATH)
        result_df = pd.read_csv(RESULT_FILE_PATH)
    except FileNotFoundError:
        print(f"âŒ ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {INPUT_FILE_PATH} ã¾ãŸã¯ {RESULT_FILE_PATH}")
        return
    except Exception as e:
        print(f"âŒ èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
        return

    # fillnaã¯æ­£è¦åŒ–ã®å‰ã«è¡Œã†
    input_df = input_df.fillna(-120.0) 
    
    rssi_cols = [c for c in input_df.columns if "rssi" in c]
    X_raw = input_df[rssi_cols].values.astype(np.float32)


    if STRATEGY == "Global":
        # å­¦ç¿’æ™‚ã®çµ±è¨ˆé‡ã‚’ãƒ­ãƒ¼ãƒ‰
        try:
            mean = np.load("models/train_mean.npy")
            std = np.load("models/train_std.npy")
            print("Strategy: Global Normalization (Using Train Stats)")
        except:
            print("âš ï¸ çµ±è¨ˆé‡ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚SubjectSpecificã«åˆ‡ã‚Šæ›¿ãˆã¾ã™ã€‚")
            mean = np.mean(X_raw, axis=0)
            std = np.std(X_raw, axis=0)
            std = np.where(std < 1e-6, 1.0, std)
    else:
        # ãã®å ´ã®ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰è¨ˆç®— (è‡ªå·±æ­£è¦åŒ–)
        mean = np.mean(X_raw, axis=0)
        std = np.std(X_raw, axis=0)
        std = np.where(std < 1e-6, 1.0, std)
        print("Strategy: Subject-Specific Normalization (Using Test Stats)")

    # æ­£è¦åŒ–é©ç”¨
    X_raw = (X_raw - mean) / std
    # â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²

    parts = ["Head", "Heart", "Rshoulder", "Lshoulder", "Rhip", "Lhip"]
    target_cols = []
    for part in parts:
        target_cols.extend([f"{part}_X", f"{part}_Z"]) 
    y_coords = result_df[target_cols].values.astype(np.float32)
    
    X_seq, y_seq = [], []
    for i in range(len(X_raw) - TIME_STEPS):
        X_seq.append(X_raw[i : i + TIME_STEPS])
        y_seq.append(y_coords[i + TIME_STEPS])
    X_seq = np.array(X_seq)
    y_seq = np.array(y_seq)
    
    # â–¼â–¼â–¼ ãƒ‡ãƒ¼ã‚¿åˆ†å‰²ã®ä¿®æ­£ (æ™‚ç³»åˆ—åˆ†å‰² & 0%å¯¾å¿œ) â–¼â–¼â–¼
    if FINE_TUNE_RATIO > 0.0:
        # æ™‚ç³»åˆ—ã‚’ç¶­æŒã—ã¦åˆ†å‰² (å‰åŠã‚’å­¦ç¿’ã€å¾ŒåŠã‚’è©•ä¾¡)
        split_idx = int(len(X_seq) * FINE_TUNE_RATIO)
        
        if split_idx == 0:
            print("âš ï¸ ãƒ‡ãƒ¼ã‚¿ãŒå°‘ãªã™ãã¾ã™ã€‚Fine-Tuningãªã—ã§è©•ä¾¡ã—ã¾ã™ã€‚")
            X_ft, y_ft = [], []
            X_eval, y_eval = X_seq, y_seq
            do_finetuning = False
        else:
            X_ft = X_seq[:split_idx]
            y_ft = y_seq[:split_idx]
            X_eval = X_seq[split_idx:]
            y_eval = y_seq[split_idx:]
            do_finetuning = True
            print(f"æ™‚ç³»åˆ—åˆ†å‰²å®Œäº†: Tuningç”¨(å‰åŠ)={len(X_ft)}, Evalç”¨(å¾ŒåŠ)={len(X_eval)}")
    else:
        # æ¯”ç‡0ãªã‚‰å…¨ã¦è©•ä¾¡ã«å›ã™
        print("è¨­å®š: Fine-Tuningãªã— (Baselineè©•ä¾¡)")
        X_ft, y_ft = [], []
        X_eval, y_eval = X_seq, y_seq
        do_finetuning = False

    # â–¼â–¼â–¼ Fine-Tuningå®Ÿè¡Œ â–¼â–¼â–¼
    if do_finetuning:
        print("\nğŸš€ Fine-Tuningé–‹å§‹...")
        # æ­£ã—ã„ã‚¯ãƒ©ã‚¹ HeatmapGenerator ã‚’ä½¿ç”¨
        ft_gen = HeatmapGenerator(X_ft, y_ft, 16, MAP_SIZE, X_RANGE, Z_RANGE)
        model.fit(ft_gen, epochs=EPOCHS, verbose=1)
        
        # ãƒ¢ãƒ‡ãƒ«ä¿å­˜ (è¨­å®šã•ã‚ŒãŸå‹•çš„ãƒ‘ã‚¹ã‚’ä½¿ç”¨)
        model.save(SAVE_MODEL_PATH)
        print(f"ğŸ’¾ Fine-Tuningæ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜ã—ã¾ã—ãŸ: {SAVE_MODEL_PATH}")
    else:
        print("Fine-Tuningã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã—ãŸã€‚")

    print("\nğŸ“Š æœ€çµ‚è©•ä¾¡ (è©•ä¾¡ç”¨ãƒ‡ãƒ¼ã‚¿)...")
    total_error = 0.0
    part_errors = {p: 0.0 for p in parts}
    count = 0
    BATCH_SIZE = 100
    
    for i in range(0, len(X_eval), BATCH_SIZE):
        end_ix = min(i + BATCH_SIZE, len(X_eval))
        X_batch = X_eval[i : end_ix]
        y_batch = y_eval[i : end_ix]
        
        preds = model.predict(X_batch, verbose=0)
        for k in range(len(preds)):
            for j, part in enumerate(parts):
                px, pz = get_coords_logic5(preds[k, :, :, j], X_RANGE, Z_RANGE)
                tx, tz = y_batch[k, j*2], y_batch[k, j*2+1]
                dist = np.sqrt((px - tx)**2 + (pz - tz)**2)
                part_errors[part] += dist
                total_error += dist
            count += 1

    if count > 0:
        avg_error = total_error / (count * len(parts))
        print("\n" + "="*50)
        print(f"ğŸ‰ æœ€çµ‚çµæœ (ID: {NAME}, Fine-Tuning: {FINE_TUNE_RATIO*100}%)")
        print("="*50)
        print(f"ğŸ† å¹³å‡èª¤å·®: {avg_error:.2f} mm")
        print("-" * 50)
        for part in parts:
            print(f"  - {part:10s}: {part_errors[part]/count:.2f} mm")
        print("="*50)
    else:
        print("è©•ä¾¡ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")

if __name__ == "__main__":
    main()